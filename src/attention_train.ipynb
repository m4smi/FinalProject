{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c653038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from jax import jit\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "from flax.training.train_state import TrainState\n",
    "from pncbf.networks.mlp import MLP\n",
    "from pncbf.networks.ncbf import SingleValueFn\n",
    "from pncbf.networks.optim import get_default_tx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dill\n",
    "from pncbf.dyn.segway import Segway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44497916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultViewAttentionCBF(nn.Module):\n",
    "\n",
    "    hidden_dim: int = 256\n",
    "    num_heads: int = 4\n",
    "    dropout_rate: float = 0.05\n",
    "    \n",
    "    def setup(self):\n",
    "        self.position_view = nn.Dense(self.hidden_dim, name='position_view')      # [p, theta]\n",
    "        self.velocity_view = nn.Dense(self.hidden_dim, name='velocity_view')      # [v, omega]  \n",
    "        self.safety_view = nn.Dense(self.hidden_dim, name='safety_view')          # [theta, omega]\n",
    "        self.full_state_view = nn.Dense(self.hidden_dim, name='full_state_view')  # [p, theta, v, omega]\n",
    "        \n",
    "        # additional views\n",
    "        self.position_coupling_view = nn.Dense(self.hidden_dim, name='pos_coupling')  # pos-vel coupling\n",
    "        self.angle_coupling_view = nn.Dense(self.hidden_dim, name='angle_coupling')   # angle-angular vel coupling\n",
    "        \n",
    "        self.self_attention = nn.MultiHeadDotProductAttention(\n",
    "            num_heads=self.num_heads,\n",
    "            qkv_features=self.hidden_dim, # W_q = W_k = W_v = [256, 256]\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            decode=False\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm()\n",
    "        \n",
    "        self.output_net = nn.Sequential([\n",
    "            nn.Dense(self.hidden_dim),\n",
    "            nn.tanh,\n",
    "            nn.Dense(self.hidden_dim // 2),\n",
    "            nn.tanh,\n",
    "            nn.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "    \n",
    "    def __call__(self, x, training: bool = True):\n",
    "        \n",
    "        # x shape: [batch_size, 4] for [p, theta, v, omega]\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        position_state = x[:, :2]   # [p, theta]\n",
    "        velocity_state = x[:, 2:]   # [v, omega]\n",
    "        \n",
    "        views = []\n",
    "        \n",
    "        # pos persective\n",
    "        pos_view = self.position_view(position_state)\n",
    "        views.append(nn.tanh(pos_view))\n",
    "        \n",
    "        # vel perspective\n",
    "        vel_view = self.velocity_view(velocity_state)\n",
    "        views.append(nn.tanh(vel_view))\n",
    "        \n",
    "        # safety perspective\n",
    "        safety_features = jnp.concatenate([\n",
    "            x[:, 1:2],  # theta \n",
    "            x[:, 3:4],  # omega \n",
    "        ], axis=1)\n",
    "\n",
    "        safety_view = self.safety_view(safety_features)\n",
    "        views.append(nn.tanh(safety_view))\n",
    "        \n",
    "        # full state persepctive\n",
    "        full_view = self.full_state_view(x)\n",
    "        views.append(nn.tanh(full_view))\n",
    "        \n",
    "        # pos-vel coupling\n",
    "        pos_vel_coupling = jnp.concatenate([\n",
    "            x[:, 0:1],  # p\n",
    "            x[:, 2:3],  # v\n",
    "        ], axis=1)\n",
    "        coupling_view = self.position_coupling_view(pos_vel_coupling)\n",
    "        views.append(nn.relu(coupling_view))\n",
    "        \n",
    "        # angle-angular vel coupling\n",
    "        angle_coupling_features = jnp.concatenate([\n",
    "            x[:, 1:2],  # theta\n",
    "            x[:, 3:4],  # omega\n",
    "        ], axis=1)\n",
    "        angle_coupling_view = self.angle_coupling_view(angle_coupling_features)\n",
    "        views.append(nn.relu(angle_coupling_view))\n",
    "        \n",
    "        # stacked views: [batch_size, num_views, hidden_dim]\n",
    "        view_stack = jnp.stack(views, axis=1)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        view_stack = self.layer_norm(view_stack)\n",
    "        \n",
    "        attended_views = self.self_attention(\n",
    "            inputs_q=view_stack,\n",
    "            inputs_kv=view_stack,\n",
    "            deterministic=not training\n",
    "        )\n",
    "        \n",
    "        # residual connection\n",
    "        attended_views = attended_views + view_stack\n",
    "        \n",
    "        # dropout during training\n",
    "        if training:\n",
    "            attended_views = self.dropout(attended_views, deterministic=False)\n",
    "        \n",
    "        # avergae the attended views \n",
    "        aggregated = jnp.mean(attended_views, axis=1)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        output = self.output_net(aggregated)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionValueFn(nn.Module):\n",
    "    hidden_dim: int = 256 \n",
    "    num_heads: int = 4\n",
    "    dropout_rate: float = 0.05\n",
    "    \n",
    "    def setup(self):\n",
    "        self.attention_net = MultViewAttentionCBF(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout_rate=self.dropout_rate\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, training: bool = True):\n",
    "        result = self.attention_net(x, training=training)\n",
    "        return result.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a521857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiview_attention_cbf():\n",
    "\n",
    "    return MultiAttentionValueFn(\n",
    "        hidden_dim=256,\n",
    "        num_heads=4,      # 6 views, so 256/4 = 64 per head\n",
    "        dropout_rate=0.05\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83b7e65",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8212162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduled_optimizer(initial_lr=1e-3):\n",
    "    \n",
    "    lr_schedule = optax.piecewise_constant_schedule(\n",
    "        init_value=initial_lr,\n",
    "        # boundaries_and_scales={\n",
    "        #     200: 0.5,   \n",
    "        #     500: 0.2,   \n",
    "        #     800: 0.5,    \n",
    "        # }\n",
    "        boundaries_and_scales={\n",
    "            100: 0.7,    \n",
    "            300: 0.5,    #  0.7 * 0.5 = 0.35 \n",
    "            600: 0.3,    # 0.35 * 0.3 = 0.105\n",
    "            900: 0.1,    # 0.105 * 0.1 = 0.0105\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return optax.adamw(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c73487",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pncbf_loss_fn(predicted_values, target_values):\n",
    "\n",
    "    loss = jnp.mean(jnp.square(predicted_values - target_values))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8631b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pncbf_scheduled(states, max_violations, network, \n",
    "                         initial_lr=1e-3, batch_size=128, epochs=1000):\n",
    "    \n",
    "    key = jax.random.PRNGKey(42)\n",
    "    dummy_input = states[0:1]\n",
    "    \n",
    "    # init with rng keys for attention + dropout\n",
    "    init_rngs = {\n",
    "        'params': key,\n",
    "        'dropout': jax.random.PRNGKey(0)\n",
    "    }\n",
    "    params = network.init(init_rngs, dummy_input)\n",
    "\n",
    "    tx = get_scheduled_optimizer(initial_lr)\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )\n",
    "    def get_current_lr(epoch):\n",
    "        # if epoch < 200:\n",
    "        #     return initial_lr\n",
    "        # elif epoch < 500:\n",
    "        #     return initial_lr * 0.5\n",
    "        # elif epoch < 800:\n",
    "        #     return initial_lr * 0.1\n",
    "        # else:\n",
    "        #     return initial_lr * 0.05\n",
    "        if epoch < 100:\n",
    "            return initial_lr\n",
    "        elif epoch < 300:\n",
    "            return initial_lr * 0.7\n",
    "        elif epoch < 600:\n",
    "            return initial_lr * 0.7 * 0.5\n",
    "        elif epoch < 900:\n",
    "            return initial_lr * 0.7 * 0.5 * 0.3\n",
    "        else:\n",
    "            return initial_lr * 0.7 * 0.5 * 0.3 * 0.1\n",
    "        \n",
    "    @jit\n",
    "    def train_step(state, batch_states, batch_values, dropout_rng):\n",
    "        def loss_fn(params):\n",
    "            predicted_values = state.apply_fn(\n",
    "                params, \n",
    "                batch_states,\n",
    "                training=True,\n",
    "                rngs={'dropout': dropout_rng}\n",
    "            )\n",
    "            return pncbf_loss_fn(predicted_values, batch_values)\n",
    "        \n",
    "        grad_fn = jax.value_and_grad(loss_fn) \n",
    "        loss, grads = grad_fn(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "        return state, loss\n",
    "    \n",
    "    losses = []\n",
    "    n_samples = len(states)\n",
    "    steps_per_epoch = n_samples // batch_size \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = jax.random.permutation(key, n_samples)\n",
    "        key, dropout_key = jax.random.split(key)\n",
    "\n",
    "        epoch_losses = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_indices = perm[step * batch_size:(step + 1) * batch_size]\n",
    "            batch_states = states[batch_indices]\n",
    "            batch_values = max_violations[batch_indices]\n",
    "\n",
    "            dropout_key, batch_dropout_key = jax.random.split(dropout_key)\n",
    "            state, loss = train_step(state, batch_states, batch_values, batch_dropout_key)\n",
    "            epoch_losses.append(loss)\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            current_lr = get_current_lr(epoch)\n",
    "            print(f\"epoch {epoch}, loss: {avg_loss:.6f}, lr: {current_lr:.2e}\")\n",
    "\n",
    "    return state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297af6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d0005",
   "metadata": {},
   "source": [
    "### Data loading and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e0fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('segway_training_data_10k.npy', allow_pickle=True).item()\n",
    "states = data['states']\n",
    "max_violations = data['violations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06be744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multiview_attention_cbf()\n",
    "\n",
    "trained_state, losses = train_pncbf_scheduled(\n",
    "    states, max_violations, model, \n",
    "    initial_lr=1e-3, epochs=1200\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Attention Training')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"final losses: {losses[-1]:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491600b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('segway_attn_6views_losses_best', losses)\n",
    "print(\"losses saved!\")\n",
    "\n",
    "# with open('pncbf_model_segway_trainstate.pkl', 'wb') as f:\n",
    "#     dill.dump(trained_state, f) #--> check later, could be bc of dropout()\n",
    "# --> params \n",
    "\n",
    "params_only = trained_state.params\n",
    "with open('segway_attn_6views_best.pkl', 'wb') as f:\n",
    "    dill.dump(params_only, f)\n",
    "\n",
    "print(\"model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
