{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c582967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import jit, vmap\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training.train_state import TrainState\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pncbf.networks.mlp import MLP\n",
    "from pncbf.networks.ncbf import SingleValueFn\n",
    "from pncbf.networks.optim import get_default_tx\n",
    "\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757af695",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp():\n",
    "\n",
    "    mlp_fn = lambda: MLP(\n",
    "        hid_sizes = (256, 256, 256),\n",
    "        act = nn.tanh,\n",
    "        act_final = True # act func() --> output layer\n",
    "    )\n",
    "\n",
    "    # wrap MLP with SingleValueFn for CBF output\n",
    "    # SingleValueFn --> ensures the network produce a single \n",
    "    # scalar output\n",
    "    return SingleValueFn(net_cls=mlp_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa2d2c",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d93de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def pncbf_loss_fn(predicted_values, target_values):\n",
    "\n",
    "    loss = jnp.mean(jnp.square(predicted_values - target_values))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af43181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pncbf(states, max_violations, network, learning_rates=1e-3, batch_size=128, epochs=1000):\n",
    "    \n",
    "    key = jax.random.PRNGKey(42)\n",
    "    dummy_input = states[0:1] # a sample for jax initialization\n",
    "    params = network.init(key, dummy_input)\n",
    "\n",
    "    tx = get_default_tx(learning_rates) # --> adamW optimizer\n",
    "\n",
    "    state = TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=params,\n",
    "        tx=tx\n",
    "    )\n",
    "    \n",
    "    @jit\n",
    "    def train_step(state, batch_states, batch_values):\n",
    "        def loss_fn(params):\n",
    "            predicted_values = state.apply_fn(params, batch_states)\n",
    "            return pncbf_loss_fn(predicted_values, batch_values)\n",
    "            # return pncbf_loss_fn(\n",
    "            #     params, \n",
    "            #     batch_states, \n",
    "            #     batch_values, \n",
    "            #     lambda p, x: state.apply_fn(p, x)\n",
    "            #     ) \n",
    "        \n",
    "        # compute gradients and update parameters\n",
    "        grad_fn = jax.value_and_grad(loss_fn) \n",
    "        loss, grads = grad_fn(state.params)\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "        return state, loss\n",
    "    \n",
    "    losses = []\n",
    "    n_samples = len(states)\n",
    "    steps_per_epoch = n_samples // batch_size \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # shuffle data at each epoch\n",
    "        perm = jax.random.permutation(key, n_samples)\n",
    "        # new random key for subsequent opers\n",
    "        key, _ = jax.random.split(key)\n",
    "\n",
    "        epoch_losses = []\n",
    "        for step in range(steps_per_epoch):\n",
    "            batch_indices = perm[step * batch_size:(step + 1) * batch_size]\n",
    "            batch_states = states[batch_indices]\n",
    "            batch_values = max_violations[batch_indices]\n",
    "\n",
    "            state, loss = train_step(state, batch_states, batch_values)\n",
    "            epoch_losses.append(loss)\n",
    "\n",
    "        avg_loss = np.mean(epoch_losses) # avg loss across all batches\n",
    "        losses.append(avg_loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"epoch {epoch}, loss: {avg_loss:.6f}\")\n",
    "\n",
    "    return state, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78511df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('segway_training_data_10k.npy', allow_pickle=True).item()\n",
    "states = data['states']\n",
    "max_violations = data['violations']\n",
    "\n",
    "MLPnn = mlp()\n",
    "\n",
    "trained_state, losses = train_pncbf(states, max_violations, MLPnn, epochs = 1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b61996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"final losses: {losses[-1]:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93400bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('segway_mlp_losses.npy', losses)\n",
    "print(\"losses saved!\")\n",
    "\n",
    "# with open('pncbf_model_segway.pkl', 'wb') as f:\n",
    "#     dill.dump(trained_state, f)\n",
    "\n",
    "params_only = trained_state.params\n",
    "with open('segway_mlp_model.pkl', 'wb') as f:\n",
    "    dill.dump(params_only, f)\n",
    "\n",
    "print(\"model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cbf310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
